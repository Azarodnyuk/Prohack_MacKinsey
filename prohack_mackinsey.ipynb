{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the datasets as pandas DataFrames\n",
    "\n",
    "sample_submit = pd.read_csv('sample_submit.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column 'y' in 'train' seemed to be the target values.\n",
    "# Here we extract the 'y'-column as 'y_train'\n",
    "\n",
    "y_train = train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a column 'galaxy' in the dataframes 'train' and 'test'\n",
    "# that has a type of 'object'.\n",
    "# To build the regression model we need to apply get_dummies to this column\n",
    "#  and put it into the separate tables for a while.\n",
    "\n",
    "\n",
    "train_dummies = pd.get_dummies(train['galaxy'])\n",
    "test_dummies = pd.get_dummies(test['galaxy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the columns 'y', 'galaxy' from both datasets (if they include one)\n",
    "\n",
    "train.drop(labels = ['y','galaxy'], axis=1, inplace = True)\n",
    "test.drop(labels = 'galaxy', axis=1, inplace = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_standard_normal_value(data, mean, std):\n",
    "    \n",
    "    \"\"\"   This function converts the 'data'  to the standard_normal form\n",
    "             with new_mean = 0 and new_std = 1   due to the formula:    \n",
    "                       z = (X - mean) / std                               \"\"\"\n",
    "    \n",
    "    z = (data - mean) / std\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For best perfomance of neural network used below,\n",
    "# we need normilize a dataset.\n",
    "# To have both datasets normilezed simular we\n",
    "# use as mean = train.mean() and as std = train.std()\n",
    "\n",
    "train_scaled = to_standard_normal_value(train, train.mean(), train.std())\n",
    "test_scaled = to_standard_normal_value(test, train.mean(), train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glue together the normilized datasets and dummy-datasets\n",
    "\n",
    "train_dummed_scaled = pd.concat([train_scaled,train_dummies], axis=1)\n",
    "test_dummed_scaled = pd.concat([test_scaled,test_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute all Nan in datasets as median in each column\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "train_dummed_scaled = imputer.fit_transform(train_dummed_scaled)\n",
    "test_dummed_scaled = imputer.fit_transform(test_dummed_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split big dataset train_dummed_scaled for train and test parts\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_dummed_scaled,y_train, test_size = 0.3, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 259)               67340     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 129)               33540     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 111,313\n",
      "Trainable params: 111,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network for regression problem\n",
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(int(train_X.shape[1]), kernel_initializer='normal',input_dim = train_X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(int(train_X.shape[1]/2), kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(int(train_X.shape[1]/4), kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(int(train_X.shape[1]/8), kernel_initializer='normal',activation='relu'))\n",
    "# NN_model.add(Dense(int(train_X.shape[1]/16), kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error']) #'mean_absolute_error'\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/conda/envs/notebook/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2164 samples, validate on 541 samples\n",
      "Epoch 1/25\n",
      "2164/2164 [==============================] - 2s 989us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 8.7968e-04 - val_mean_squared_error: 8.7968e-04\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00088, saving model to Weights-001--0.00088.hdf5\n",
      "Epoch 2/25\n",
      "2164/2164 [==============================] - 2s 868us/step - loss: 5.2058e-04 - mean_squared_error: 5.2058e-04 - val_loss: 5.3961e-04 - val_mean_squared_error: 5.3961e-04\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00088 to 0.00054, saving model to Weights-002--0.00054.hdf5\n",
      "Epoch 3/25\n",
      "2164/2164 [==============================] - 2s 876us/step - loss: 3.7000e-04 - mean_squared_error: 3.7000e-04 - val_loss: 3.9465e-04 - val_mean_squared_error: 3.9465e-04\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00054 to 0.00039, saving model to Weights-003--0.00039.hdf5\n",
      "Epoch 4/25\n",
      "2164/2164 [==============================] - 2s 870us/step - loss: 2.5146e-04 - mean_squared_error: 2.5146e-04 - val_loss: 2.6575e-04 - val_mean_squared_error: 2.6575e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00039 to 0.00027, saving model to Weights-004--0.00027.hdf5\n",
      "Epoch 5/25\n",
      "2164/2164 [==============================] - 2s 867us/step - loss: 3.2939e-04 - mean_squared_error: 3.2939e-04 - val_loss: 5.0682e-04 - val_mean_squared_error: 5.0682e-04\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00027\n",
      "Epoch 6/25\n",
      "2164/2164 [==============================] - 2s 874us/step - loss: 3.0796e-04 - mean_squared_error: 3.0796e-04 - val_loss: 4.3503e-04 - val_mean_squared_error: 4.3503e-04\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00027\n",
      "Epoch 7/25\n",
      "2164/2164 [==============================] - 2s 919us/step - loss: 1.7036e-04 - mean_squared_error: 1.7036e-04 - val_loss: 3.6071e-04 - val_mean_squared_error: 3.6071e-04\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00027\n",
      "Epoch 8/25\n",
      "2164/2164 [==============================] - 2s 923us/step - loss: 1.4504e-04 - mean_squared_error: 1.4504e-04 - val_loss: 5.4767e-04 - val_mean_squared_error: 5.4767e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00027\n",
      "Epoch 9/25\n",
      "2164/2164 [==============================] - 2s 926us/step - loss: 2.0334e-04 - mean_squared_error: 2.0334e-04 - val_loss: 4.1172e-04 - val_mean_squared_error: 4.1172e-04\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00027\n",
      "Epoch 10/25\n",
      "2164/2164 [==============================] - 2s 953us/step - loss: 1.5129e-04 - mean_squared_error: 1.5129e-04 - val_loss: 2.5928e-04 - val_mean_squared_error: 2.5928e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00027 to 0.00026, saving model to Weights-010--0.00026.hdf5\n",
      "Epoch 11/25\n",
      "2164/2164 [==============================] - 2s 923us/step - loss: 1.4837e-04 - mean_squared_error: 1.4837e-04 - val_loss: 3.0436e-04 - val_mean_squared_error: 3.0436e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00026\n",
      "Epoch 12/25\n",
      "2164/2164 [==============================] - 2s 918us/step - loss: 1.8296e-04 - mean_squared_error: 1.8296e-04 - val_loss: 2.9510e-04 - val_mean_squared_error: 2.9510e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00026\n",
      "Epoch 13/25\n",
      "2164/2164 [==============================] - 2s 875us/step - loss: 1.4191e-04 - mean_squared_error: 1.4191e-04 - val_loss: 5.0297e-04 - val_mean_squared_error: 5.0297e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00026\n",
      "Epoch 14/25\n",
      "2164/2164 [==============================] - 2s 878us/step - loss: 2.2294e-04 - mean_squared_error: 2.2294e-04 - val_loss: 3.2673e-04 - val_mean_squared_error: 3.2673e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00026\n",
      "Epoch 15/25\n",
      "2164/2164 [==============================] - 2s 872us/step - loss: 1.0469e-04 - mean_squared_error: 1.0469e-04 - val_loss: 2.4482e-04 - val_mean_squared_error: 2.4482e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00026 to 0.00024, saving model to Weights-015--0.00024.hdf5\n",
      "Epoch 16/25\n",
      "2164/2164 [==============================] - 2s 872us/step - loss: 6.6318e-05 - mean_squared_error: 6.6318e-05 - val_loss: 2.6096e-04 - val_mean_squared_error: 2.6096e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00024\n",
      "Epoch 17/25\n",
      "2164/2164 [==============================] - 2s 878us/step - loss: 6.1930e-05 - mean_squared_error: 6.1930e-05 - val_loss: 2.6578e-04 - val_mean_squared_error: 2.6578e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00024\n",
      "Epoch 18/25\n",
      "2164/2164 [==============================] - 2s 898us/step - loss: 8.3279e-05 - mean_squared_error: 8.3279e-05 - val_loss: 2.6564e-04 - val_mean_squared_error: 2.6564e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00024\n",
      "Epoch 19/25\n",
      "2164/2164 [==============================] - 2s 876us/step - loss: 4.6339e-05 - mean_squared_error: 4.6339e-05 - val_loss: 1.9863e-04 - val_mean_squared_error: 1.9863e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00024 to 0.00020, saving model to Weights-019--0.00020.hdf5\n",
      "Epoch 20/25\n",
      "2164/2164 [==============================] - 2s 866us/step - loss: 7.2054e-05 - mean_squared_error: 7.2054e-05 - val_loss: 3.0503e-04 - val_mean_squared_error: 3.0503e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00020\n",
      "Epoch 21/25\n",
      "2164/2164 [==============================] - 2s 882us/step - loss: 6.3888e-05 - mean_squared_error: 6.3888e-05 - val_loss: 3.0027e-04 - val_mean_squared_error: 3.0027e-04 - loss: 6.8883e-05 - mean_squared_error: \n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00020\n",
      "Epoch 22/25\n",
      "2164/2164 [==============================] - 2s 876us/step - loss: 1.2959e-04 - mean_squared_error: 1.2959e-04 - val_loss: 3.1498e-04 - val_mean_squared_error: 3.1498e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00020\n",
      "Epoch 23/25\n",
      "2164/2164 [==============================] - 2s 879us/step - loss: 5.6083e-05 - mean_squared_error: 5.6083e-05 - val_loss: 5.1739e-04 - val_mean_squared_error: 5.1739e-04oss: 6.9963e-05 - mean_squ\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00020\n",
      "Epoch 24/25\n",
      "2164/2164 [==============================] - 2s 871us/step - loss: 4.0581e-05 - mean_squared_error: 4.0581e-05 - val_loss: 2.7914e-04 - val_mean_squared_error: 2.7914e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00020\n",
      "Epoch 25/25\n",
      "2164/2164 [==============================] - 2s 843us/step - loss: 4.1207e-05 - mean_squared_error: 4.1207e-05 - val_loss: 3.1896e-04 - val_mean_squared_error: 3.1896e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f9afeea3950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(train_X, train_y, epochs=25, batch_size=10, validation_split = 0.2, callbacks=callbacks_list,\n",
    "             workers=20, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = NN_model.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011951943930397734"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(val_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
